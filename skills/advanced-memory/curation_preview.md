# Memory Curation Preview
**Generated:** 2026-02-02 10:01:26 CST
**Source:** Advanced-memory database (importance >= 7)
**Total extracted:** 17 unique memories

---

## Proposed MEMORY.md Additions
*Sort: importance desc → date asc*

### Importance: 8

- 2026-02-01 — Built advanced-memory skill with LanceDB vector database and Ollama embeddings for semantic memory search (importance: 8) [source: advanced-memory:mem_1769957402622]
- 2026-02-01 — Created web UI for advanced-memory at port 8768 with dark theme, semantic search, filters, and add memory modal (importance: 8) [source: advanced-memory:mem_1769957402657]
- 2026-02-01 — System has dual NVIDIA GeForce RTX 5060 Ti GPUs with 16GB VRAM each (32GB total GPU memory) (importance: 8) [source: advanced-memory:mem_1769957830208]
- 2026-02-01 — CPU: AMD Ryzen 9 9900X3D 12-Core (24 threads), 91GB system RAM (importance: 8) [source: advanced-memory:mem_1769957830225]
- 2026-02-01 — Ollama is running on both GPUs with 80+ models available locally, including large models up to 87GB (MiniMax-M2.1-REAP-30) (importance: 8) [source: advanced-memory:mem_1769957830242]
- 2026-02-01 — Whisper transcription service being built - GPU-accelerated audio-to-text using faster-whisper, will auto-store transcriptions as memories (importance: 8) [source: advanced-memory:mem_1769958969495]

### Importance: 7

- 2026-02-01 — Fixed LanceDB Float32 type error by using PyArrow schema with explicit float32 vector type definition (importance: 7) [source: advanced-memory:mem_1769957402641]
- 2026-02-01 — User's system is accessible at 192.168.2.22 from external machines, not localhost (importance: 7) [source: advanced-memory:mem_1769957402688]
- 2026-02-01 — Advanced-memory skill lives at ~/.openclaw/workspace/skills/advanced-memory and survives OpenClaw updates (importance: 7) [source: advanced-memory:mem_1769957402673]
- 2026-02-01 — Context: User has a 3CX cloud account and wants to self-host it locally in Docker. Also has a Google Voice number. (importance: 7) [source: advanced-memory:mem_1769957565755]
- 2026-02-01 — Example: "VoIP projects" → found "User wants to self-host 3CX for VoIP" (similarity: 469.8) (importance: 7) [source: advanced-memory:mem_1769957565764]
- 2026-02-01 — Session notes: Good morning session - maintenance + project planning. Advanced memory skill is 90% done, just needs final type conversion fix. (importance: 7) [source: advanced-memory:mem_1769957565772]
- 2026-02-01 — Current embedding model: nomic-embed-text (274MB), but larger embeddings available like snowflake-arctic-embed-m-long and mxbai-embed-large (importance: 7) [source: advanced-memory:mem_1769957830259]
- 2026-02-01 — System has multiple uncensored/creative models for generating descriptions: stheno-v3.2, darkidol, mythomax, ninja-v3 (importance: 7) [source: advanced-memory:mem_1769957830275]
- 2026-02-01 — User approved expanding advanced-memory with local GPU features while keeping 100% offline (no cloud API calls) (importance: 7) [source: advanced-memory:mem_1769958969528]
- 2026-02-01 — Advanced-memory expansion plan: Phase 1 LLM scoring (phi-3.5/qwen2.5), Phase 2 Whisper audio transcription, Phase 3 RAG Q&A, Phase 4 summarization, Phase 5 entity extraction (importance: 7) [source: advanced-memory:mem_1769958969511]
- 2026-02-01 — Curate command parses markdown files, extracts important chunks, auto-scores and stores memories above threshold (importance: 7) [source: advanced-memory:mem_1769958969579]

---

## Statistics

- **Total memories with importance >= 7:** 17
- **Importance 8:** 6 items
- **Importance 7:** 11 items
- **Date range:** 2026-02-01 only
- **Tiers:** global (15), user (2)

## Top 5 Examples

1. **[imp:8]** Built advanced-memory skill with LanceDB vector database and Ollama embeddings for semantic memory search
2. **[imp:8]** Created web UI for advanced-memory at port 8768 with dark theme, semantic search, filters, and add memory modal
3. **[imp:8]** System has dual NVIDIA GeForce RTX 5060 Ti GPUs with 16GB VRAM each (32GB total GPU memory)
4. **[imp:8]** CPU: AMD Ryzen 9 9900X3D 12-Core (24 threads), 91GB system RAM
5. **[imp:8]** Ollama is running on both GPUs with 80+ models available locally, including large models up to 87GB

---

## Next Steps

1. **Review** this preview for relevance and accuracy
2. **Edit** MEMORY.md manually to append selected entries (or edit this file and apply as patch)
3. **Commit** the updated MEMORY.md to git
4. **Optional:** Run with lower threshold (e.g., importance >= 6) if more context needed
